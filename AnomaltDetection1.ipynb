{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65bdfaf8-3794-42eb-b9aa-94df1e55d35a",
   "metadata": {},
   "source": [
    "Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e88ce66-c125-4e55-ab49-c63b46571df6",
   "metadata": {},
   "source": [
    "Anomaly detection identifies data points that deviate significantly from the norm, indicating potential errors, fraud, or rare events. Its purpose spans various fields such as fraud detection, network security, predictive maintenance, healthcare, and environmental monitoring, aiming to uncover unusual patterns and prevent adverse outcomes. Techniques include statistical methods and machine learning approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b33cf4-a045-43fe-af99-913fc2fcfe2e",
   "metadata": {},
   "source": [
    "Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e39e516-834b-4451-b7ea-60c6e970cdaf",
   "metadata": {},
   "source": [
    "The key challenges in anomaly detection include:\n",
    "\n",
    "Imbalanced Data: Anomalies are typically rare compared to normal instances, making it difficult to train models effectively.\n",
    "- High Dimensionality: Data with many features can complicate the identification of relevant anomalies.\n",
    "- Dynamic and Evolving Data: Changing patterns over time can make it hard to distinguish between normal behavior and anomalies.\n",
    "- Noise: Distinguishing true anomalies from noise or irrelevant variations in the data can be challenging.\n",
    "- Lack of Labeled Data: Often, there is a scarcity of labeled anomalies for supervised learning approaches.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97254e12-4c11-4ae2-86ea-c423fc2b9676",
   "metadata": {},
   "source": [
    "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec1a5b-6be0-427e-b52c-a2224ff3a5b0",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection identifies anomalies without labeled data by finding patterns that deviate significantly from the norm using techniques like clustering and statistical models. Supervised anomaly detection, on the other hand, relies on labeled data to train models that distinguish between normal and anomalous instances, typically using classification algorithms. The main difference lies in the availability and use of labeled training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf3d44a-bb8b-440c-a572-9ab59e7e9109",
   "metadata": {},
   "source": [
    "Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7729317-332a-42bc-9b2c-4ede8c9a757c",
   "metadata": {},
   "source": [
    "The main categories of anomaly detection algorithms are:\n",
    "\n",
    "- Statistical Methods: Identify anomalies based on statistical properties, assuming data follows a specific distribution (e.g., Z-Score, Grubbs' Test).\n",
    "- Machine Learning Methods:\n",
    "    - Supervised Methods: Use labeled data to train models that classify instances as normal or anomalous (e.g., decision trees, SVM).\n",
    "    - Unsupervised Methods: Detect anomalies without labeled data by identifying deviations from normal patterns (e.g., clustering, principal component analysis).\n",
    "    - Semi-Supervised Methods: Use a mix of labeled normal data and unlabeled data to identify anomalies (e.g., one-class SVM, autoencoders).\n",
    "- Proximity-Based Methods: Identify anomalies based on distance or density measures (e.g., k-nearest neighbors, DBSCAN).\n",
    "- Ensemble Methods: Combine multiple algorithms to improve detection accuracy and robustness (e.g., isolation forest, ensemble learning techniques)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297aca18-334c-446d-83ba-988bc98fffb4",
   "metadata": {},
   "source": [
    "Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433ef4c-1a1e-4b43-bee7-c184981b7899",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods make several key assumptions:\n",
    "\n",
    "1. **Homogeneity**: Normal data points are clustered together, exhibiting higher density, while anomalies are isolated and distant from these clusters.\n",
    "2. **Distance Metric**: The choice of distance metric (e.g., Euclidean, Manhattan) appropriately captures the similarity or dissimilarity between data points.\n",
    "3. **Scale**: Features are scaled similarly, as varying scales can disproportionately affect distance calculations.\n",
    "4. **Static Data Distribution**: The underlying data distribution remains relatively stable over time, ensuring that distance metrics remain consistent in identifying anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e31c7-88ea-4c48-babe-f7d642413b29",
   "metadata": {},
   "source": [
    "Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8a7d1-2ca3-4a03-af83-25c3671888d2",
   "metadata": {},
   "source": [
    "Using K Nearest Neighbour algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d969f0cd-58ae-4f23-a614-ee52501a4948",
   "metadata": {},
   "source": [
    "Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b44489-0289-4d5f-91a0-558be84e95ad",
   "metadata": {},
   "source": [
    "The key parameters of the Isolation Forest algorithm are:\n",
    "\n",
    "- n_estimators: The number of trees in the forest. More trees can improve the robustness of the anomaly detection but also increase computational cost.\n",
    "- max_samples: The number of samples to be drawn from the dataset to train each tree. It can be an integer or a fraction. Using a smaller subset can speed up computation.\n",
    "- contamination: The expected proportion of anomalies in the dataset. It helps to define the threshold on the anomaly score.\n",
    "- max_features: The number of features to consider when splitting a node. It can be an integer or a fraction, influencing the diversity and performance of trees.\n",
    "- bootstrap: Whether samples are drawn with replacement. Bootstrapping can introduce variability among the trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b724ddc-038d-4067-9eb1-32493c412c65",
   "metadata": {},
   "source": [
    "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0983d3-ce3d-442c-8f7d-bbd865f90866",
   "metadata": {},
   "source": [
    "insuff data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc27119-8896-44a0-97a7-63c897cd1d97",
   "metadata": {},
   "source": [
    "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "length of the trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de91bb7-c456-4c54-aa45-45650d1a648b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
