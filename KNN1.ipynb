{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9249176-7fd7-4cf1-905a-de8f723b9e86",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb9165-990d-4587-be55-828208d1d527",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a simple yet effective supervised learning algorithm used for classification and regression tasks. It works by finding the k closest data points in the training set to a given test data point and then assigns the most common class label (for classification) or the average value (for regression) among those k neighbors to the test data point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788310be-41e9-4e5d-b155-2aa3ff070467",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbfe211-01d6-45fb-aabb-60a008b0657d",
   "metadata": {},
   "source": [
    "The value of k in K-Nearest Neighbors (KNN) is typically chosen using techniques like cross-validation or grid search. Cross-validation involves splitting the data into training and validation sets multiple times, evaluating the model's performance with different k values, and selecting the one that gives the best performance. Grid search exhaustively searches through a specified range of k values and evaluates each one using a specified performance metric, such as accuracy or F1 score, to determine the optimal k value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6866a5b3-8298-45c0-af53-2f26823e92a0",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d2067e-7ff1-4755-99e2-3f3b6d3f67ff",
   "metadata": {},
   "source": [
    "The main difference between a KNN classifier and a KNN regressor lies in their output types. A KNN classifier predicts discrete class labels for classification tasks, while a KNN regressor predicts continuous values for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cdaea4-e7c2-40c6-8778-e797ff40e73b",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e833b1-90db-4d4d-afac-bf32cb9d59a1",
   "metadata": {},
   "source": [
    "The performance of a K-Nearest Neighbors (KNN) model can be measured using various metrics such as accuracy, precision, recall, F1 score, and the confusion matrix for classification tasks. For regression tasks, metrics like mean squared error (MSE), mean absolute error (MAE), and R-squared can be used to assess the model's performance. Cross-validation techniques like k-fold cross-validation can also be employed to get a more robust evaluation of the model's performance across different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cadd3b-6a96-435c-ac1b-b6f6daefac02",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b650e6fd-1c01-4088-a2e9-47ed8d386665",
   "metadata": {},
   "source": [
    "The curse of dimensionality refers to the challenge faced by K-Nearest Neighbors (KNN) and other algorithms when working with high-dimensional data. As the number of features (dimensions) increases, the volume of the space increases exponentially, leading to sparsity in the data. This sparsity can cause the nearest neighbors to be far away in high-dimensional space, reducing the effectiveness of the KNN algorithm and increasing computational complexity. Dimensionality reduction techniques like PCA or feature selection can help mitigate this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a205a9c-ad8f-40bc-b069-a7c1bcac9919",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02452558-5222-4b0f-be79-84c24b621b67",
   "metadata": {},
   "source": [
    "Handling missing values in K-Nearest Neighbors (KNN) involves imputation, where missing values are replaced with estimated values based on the available data. One common approach is to replace missing values with the mean, median, or mode of the feature across the dataset. Alternatively, you can use algorithms like k-NN imputation, which uses the k nearest neighbors to predict missing values based on similar instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7461a6-dc64-4c83-9b69-4967890b4657",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc71a1b4-9496-495b-8684-50ad4fbedeba",
   "metadata": {},
   "source": [
    "The performance of a K-Nearest Neighbors (KNN) classifier and regressor depends on the nature of the problem.\n",
    "\n",
    "- KNN Classifier: It works well for classification tasks where the output is discrete class labels. It's suitable for problems like sentiment analysis, spam detection, and image classification.\n",
    "- KNN Regressor: It's effective for regression tasks where the output is continuous values. It's useful for predicting quantities like stock prices, housing prices, and temperature forecasting.\n",
    "\n",
    "\n",
    "The choice between classifier and regressor depends on the problem's output type: discrete (classifier) or continuous (regressor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa292a85-cce7-4162-8b4a-f4f1a1b19d4d",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8b017d-9b66-42ea-a94e-2046adfb9827",
   "metadata": {},
   "source": [
    "Strengths of KNN Algorithm:\n",
    "\n",
    "- Simplicity: Easy to understand and implement.\n",
    "- Non-parametric: Doesn't assume any underlying distribution of the data.\n",
    "- Adaptability: Can handle complex decision boundaries.\n",
    "\n",
    "\n",
    "\n",
    "Weaknesses of KNN Algorithm:\n",
    "    \n",
    "- Computational Cost: High computational cost, especially with large datasets.\n",
    "- Sensitivity to Noise and Outliers: Prone to influence from noisy data points.\n",
    "- Need for Feature Scaling: Requires feature scaling for optimal performance.\n",
    "\n",
    "\n",
    "\n",
    "These weaknesses can be addressed by:\n",
    "\n",
    "- Optimizing Parameters: Choosing the right value of k and using distance metrics wisely.\n",
    "-Dimensionality Reduction: Using techniques like PCA to reduce the number of features.\n",
    "-Outlier Handling: Removing or correcting outliers to reduce their impact.\n",
    "-Parallelization: Utilizing parallel processing to improve computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be3048-3691-46f0-8cdb-1ce87f288619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
